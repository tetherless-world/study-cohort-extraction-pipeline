{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Pipeline\n",
    "\n",
    "Run the cells in this notebook to, given an input JSON file, output 3 files: an RDF knowledge graph, a visualization of which tokens were identified, and a .CSV file used for results analysis.\n",
    "\n",
    "input_file should be a string of the filepath to the .JSON extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data/input/nejmoa1615692_appendix_to_correct.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction is loaded with tree_table_extraction.load_extraction(input_file), and then passed to tree_table_extraction.make_tree_tables to set generate the tree table dictionary. This dictionary is stored in data.\n",
    "\n",
    "Then, the KG Builder is created. We call build_kg(data) to build a preliminary KG structure inside of the KG_Builder object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded extraction from ./data/input/nejmoa1615692_appendix_to_correct.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from extraction.kg_builder import KG_Builder\n",
    "from extraction.tree_table_extraction import load_extraction, make_tree_tables\n",
    "from extraction.classifiers import *\n",
    "\n",
    "# Add more description\n",
    "# Add requirements.txt\n",
    "# Auto-generation tools for creating package dependencies\n",
    "# Add high-level description to readme\n",
    "#  Add workflow image to readme\n",
    "\n",
    "# first, we create the tree tables from the .JSONs using the make_tree_tables function (Step 2)\n",
    "data = make_tree_tables(load_extraction(input_file))\n",
    "\n",
    "# now we feed that data into the KG builder\n",
    "# Step 3 and 4 (includes NCBO annotation)\n",
    "kg = KG_Builder()\n",
    "\n",
    "# Document priority of classifiers\n",
    "# Document how to configure list of ontologies\n",
    "# Document additional configurable properties \n",
    "#   Include different configurable options in a script -- dont overly complicate\n",
    "kg.TokenClassifiers = [Free_Value_Token_Classifier(), Concept_Token_Classifier()]\n",
    "\n",
    "kg.build_KG(data)\n",
    "\n",
    "# Move helper code to serialize to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following contains helper code for generating the KG, visualization, and CSV file. By using set_text, we can set the text of the table object to show what the KG Builder has annotated, as well as setting the csv_rows array used for generating the results CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction import serialize\n",
    "from extraction import visualize\n",
    "\n",
    "serialize.print_graph(data,input_file)\n",
    "visualize.print_visualization(data,input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
